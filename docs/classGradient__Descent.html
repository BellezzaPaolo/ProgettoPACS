<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ParaflowS PINN: Gradient_Descent&lt; NetT &gt; Class Template Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">ParaflowS PINN
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="classGradient__Descent-members.html">List of all members</a>  </div>
  <div class="headertitle"><div class="title">Gradient_Descent&lt; NetT &gt; Class Template Reference</div></div>
</div><!--header-->
<div class="contents">

<p>Gradient Descent optimizer/trainer for the torch PINN pipeline.  
 <a href="classGradient__Descent.html#details">More...</a></p>

<p><code>#include &lt;<a class="el" href="Gradient__Descent_8hpp_source.html">Gradient_Descent.hpp</a>&gt;</code></p>
<div class="dynheader">
Inheritance diagram for Gradient_Descent&lt; NetT &gt;:</div>
<div class="dyncontent">
<div class="center"><img src="classGradient__Descent__inherit__graph.png" border="0" usemap="#aGradient__Descent_3_01NetT_01_4_inherit__map" alt="Inheritance graph"/></div>
<map name="aGradient__Descent_3_01NetT_01_4_inherit__map" id="aGradient__Descent_3_01NetT_01_4_inherit__map">
<area shape="rect" title="Gradient Descent optimizer/trainer for the torch PINN pipeline." alt="" coords="5,79,204,104"/>
<area shape="rect" href="classOptimizer.html" title="Abstract class base for optimizer/trainer for the torch PINN pipeline." alt="" coords="31,5,178,31"/>
<area shape="poly" title=" " alt="" coords="107,44,107,79,102,79,102,44"/>
</map>
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<div class="dynheader">
Collaboration diagram for Gradient_Descent&lt; NetT &gt;:</div>
<div class="dyncontent">
<div class="center"><img src="classGradient__Descent__coll__graph.png" border="0" usemap="#aGradient__Descent_3_01NetT_01_4_coll__map" alt="Collaboration graph"/></div>
<map name="aGradient__Descent_3_01NetT_01_4_coll__map" id="aGradient__Descent_3_01NetT_01_4_coll__map">
<area shape="rect" title="Gradient Descent optimizer/trainer for the torch PINN pipeline." alt="" coords="5,79,204,104"/>
<area shape="rect" href="classOptimizer.html" title="Abstract class base for optimizer/trainer for the torch PINN pipeline." alt="" coords="31,5,178,31"/>
<area shape="poly" title=" " alt="" coords="107,44,107,79,102,79,102,44"/>
</map>
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:af63b82b01d38205d0d7c7f182ba3ea57" id="r_af63b82b01d38205d0d7c7f182ba3ea57"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classGradient__Descent.html#af63b82b01d38205d0d7c7f182ba3ea57">Gradient_Descent</a> (std::shared_ptr&lt; <a class="el" href="classPde.html">Pde</a> &gt; data, std::shared_ptr&lt; NetT &gt; net, double lr, std::function&lt; tensor(const tensor &amp;)&gt; loss_fn)</td></tr>
<tr class="memdesc:af63b82b01d38205d0d7c7f182ba3ea57"><td class="mdescLeft">&#160;</td><td class="mdescRight">Construct a Gradient Descent optimizer.  <br /></td></tr>
<tr class="separator:af63b82b01d38205d0d7c7f182ba3ea57"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a91b2774d64fc86bec0b0f923fb0716c9" id="r_a91b2774d64fc86bec0b0f923fb0716c9"><td class="memItemLeft" align="right" valign="top"><a class="el" href="structResult.html">Result</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classGradient__Descent.html#a91b2774d64fc86bec0b0f923fb0716c9">train</a> (int batch_size, int budget, int max_iterations, bool verbose) override</td></tr>
<tr class="memdesc:a91b2774d64fc86bec0b0f923fb0716c9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Train using SGD.  <br /></td></tr>
<tr class="separator:a91b2774d64fc86bec0b0f923fb0716c9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classOptimizer"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classOptimizer')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classOptimizer.html">Optimizer&lt; NetT &gt;</a></td></tr>
<tr class="memitem:a5b5c97ccf1282712b3bb08c1bd969a6d inherit pub_methods_classOptimizer" id="r_a5b5c97ccf1282712b3bb08c1bd969a6d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classOptimizer.html#a5b5c97ccf1282712b3bb08c1bd969a6d">Optimizer</a> (std::shared_ptr&lt; <a class="el" href="classPde.html">Pde</a> &gt; data, std::shared_ptr&lt; NetT &gt; net, std::function&lt; tensor(const tensor &amp;)&gt; loss_fn)</td></tr>
<tr class="memdesc:a5b5c97ccf1282712b3bb08c1bd969a6d inherit pub_methods_classOptimizer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Construct an optimizer bound to a dataset and a network.  <br /></td></tr>
<tr class="separator:a5b5c97ccf1282712b3bb08c1bd969a6d inherit pub_methods_classOptimizer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab6e0d6411bed20785b47ec843c3704f4 inherit pub_methods_classOptimizer" id="r_ab6e0d6411bed20785b47ec843c3704f4"><td class="memItemLeft" align="right" valign="top">const std::vector&lt; double &gt; &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classOptimizer.html#ab6e0d6411bed20785b47ec843c3704f4">get_loss_history_train</a> () const</td></tr>
<tr class="memdesc:ab6e0d6411bed20785b47ec843c3704f4 inherit pub_methods_classOptimizer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return the training loss history.  <br /></td></tr>
<tr class="separator:ab6e0d6411bed20785b47ec843c3704f4 inherit pub_methods_classOptimizer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4ded6e28243831292ede058be54d9dc1 inherit pub_methods_classOptimizer" id="r_a4ded6e28243831292ede058be54d9dc1"><td class="memItemLeft" align="right" valign="top">const std::vector&lt; double &gt; &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classOptimizer.html#a4ded6e28243831292ede058be54d9dc1">get_loss_history_test</a> () const</td></tr>
<tr class="memdesc:a4ded6e28243831292ede058be54d9dc1 inherit pub_methods_classOptimizer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return the test loss history.  <br /></td></tr>
<tr class="separator:a4ded6e28243831292ede058be54d9dc1 inherit pub_methods_classOptimizer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a85f3d74ddbccf8bc3d8a78aa4f0d8f5c inherit pub_methods_classOptimizer" id="r_a85f3d74ddbccf8bc3d8a78aa4f0d8f5c"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classOptimizer.html#a85f3d74ddbccf8bc3d8a78aa4f0d8f5c">plot_cost_history</a> () const</td></tr>
<tr class="memdesc:a85f3d74ddbccf8bc3d8a78aa4f0d8f5c inherit pub_methods_classOptimizer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Plot the stored cost history using Python/matplotlib.  <br /></td></tr>
<tr class="separator:a85f3d74ddbccf8bc3d8a78aa4f0d8f5c inherit pub_methods_classOptimizer"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="inherited" name="inherited"></a>
Additional Inherited Members</h2></td></tr>
<tr class="inherit_header pro_attribs_classOptimizer"><td colspan="2" onclick="javascript:toggleInherit('pro_attribs_classOptimizer')"><img src="closed.png" alt="-"/>&#160;Protected Attributes inherited from <a class="el" href="classOptimizer.html">Optimizer&lt; NetT &gt;</a></td></tr>
<tr class="memitem:ace1a5e5e0f170e04d587d9f68bcd4475 inherit pro_attribs_classOptimizer" id="r_ace1a5e5e0f170e04d587d9f68bcd4475"><td class="memItemLeft" align="right" valign="top">
std::shared_ptr&lt; <a class="el" href="classPde.html">Pde</a> &gt;&#160;</td><td class="memItemRight" valign="bottom"><b>data</b></td></tr>
<tr class="separator:ace1a5e5e0f170e04d587d9f68bcd4475 inherit pro_attribs_classOptimizer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3a600cfdbef2f6ab413070f5eef7a2e2 inherit pro_attribs_classOptimizer" id="r_a3a600cfdbef2f6ab413070f5eef7a2e2"><td class="memItemLeft" align="right" valign="top">
std::shared_ptr&lt; NetT &gt;&#160;</td><td class="memItemRight" valign="bottom"><b>net</b></td></tr>
<tr class="separator:a3a600cfdbef2f6ab413070f5eef7a2e2 inherit pro_attribs_classOptimizer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aea5da08b19f1fc1bc2b1c0c687849717 inherit pro_attribs_classOptimizer" id="r_aea5da08b19f1fc1bc2b1c0c687849717"><td class="memItemLeft" align="right" valign="top">
std::function&lt; tensor(const tensor &amp;)&gt;&#160;</td><td class="memItemRight" valign="bottom"><b>loss_fn</b></td></tr>
<tr class="separator:aea5da08b19f1fc1bc2b1c0c687849717 inherit pro_attribs_classOptimizer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a65e102d41563bf228c653dec470d0611 inherit pro_attribs_classOptimizer" id="r_a65e102d41563bf228c653dec470d0611"><td class="memItemLeft" align="right" valign="top">std::vector&lt; double &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classOptimizer.html#a65e102d41563bf228c653dec470d0611">loss_history_train</a></td></tr>
<tr class="memdesc:a65e102d41563bf228c653dec470d0611 inherit pro_attribs_classOptimizer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Loss histories for plotting/diagnostics.  <br /></td></tr>
<tr class="separator:a65e102d41563bf228c653dec470d0611 inherit pro_attribs_classOptimizer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af7a772fa956f8d02ffe71cded1e6d735 inherit pro_attribs_classOptimizer" id="r_af7a772fa956f8d02ffe71cded1e6d735"><td class="memItemLeft" align="right" valign="top">
std::vector&lt; double &gt;&#160;</td><td class="memItemRight" valign="bottom"><b>loss_history_test</b></td></tr>
<tr class="separator:af7a772fa956f8d02ffe71cded1e6d735 inherit pro_attribs_classOptimizer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa4d8b3454242a31856804004ff1a2c73 inherit pro_attribs_classOptimizer" id="r_aa4d8b3454242a31856804004ff1a2c73"><td class="memItemLeft" align="right" valign="top">
bool&#160;</td><td class="memItemRight" valign="bottom"><b>use_test</b></td></tr>
<tr class="separator:aa4d8b3454242a31856804004ff1a2c73 inherit pro_attribs_classOptimizer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aaad7549516bf5646ca666f77f88d20d4 inherit pro_attribs_classOptimizer" id="r_aaad7549516bf5646ca666f77f88d20d4"><td class="memItemLeft" align="right" valign="top">
int&#160;</td><td class="memItemRight" valign="bottom"><b>budget_used</b> = 0</td></tr>
<tr class="separator:aaad7549516bf5646ca666f77f88d20d4 inherit pro_attribs_classOptimizer"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><div class="compoundTemplParams">template&lt;class NetT&gt;<br />
class Gradient_Descent&lt; NetT &gt;</div><p>Gradient Descent optimizer/trainer for the torch PINN pipeline. </p>
<p>This implements the Gradient Descent scheme: This optimizer performs standard stochastic gradient descent using <code>torch::optim::SGD</code> over the network parameters.</p>
<p>The interface matches the other project optimizers: it derives from <code><a class="el" href="classOptimizer.html" title="Abstract class base for optimizer/trainer for the torch PINN pipeline.">Optimizer</a>&lt;NetT&gt;</code> and implements <code><a class="el" href="classGradient__Descent.html#a91b2774d64fc86bec0b0f923fb0716c9" title="Train using SGD.">train()</a></code>. </p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="af63b82b01d38205d0d7c7f182ba3ea57" name="af63b82b01d38205d0d7c7f182ba3ea57"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af63b82b01d38205d0d7c7f182ba3ea57">&#9670;&#160;</a></span>Gradient_Descent()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class NetT &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classGradient__Descent.html">Gradient_Descent</a>&lt; NetT &gt;<a class="el" href="classGradient__Descent.html">::Gradient_Descent</a> </td>
          <td>(</td>
          <td class="paramtype">std::shared_ptr&lt; <a class="el" href="classPde.html">Pde</a> &gt;&#160;</td>
          <td class="paramname"><em>data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::shared_ptr&lt; NetT &gt;&#160;</td>
          <td class="paramname"><em>net</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>lr</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::function&lt; tensor(const tensor &amp;)&gt;&#160;</td>
          <td class="paramname"><em>loss_fn</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Construct a Gradient Descent optimizer. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">data</td><td>Dataset/geometry wrapper. </td></tr>
    <tr><td class="paramname">net</td><td>Network to optimize. </td></tr>
    <tr><td class="paramname">lr</td><td>Learning rate. </td></tr>
    <tr><td class="paramname">loss_fn</td><td>Scalar loss reducer (e.g. MSE). </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a91b2774d64fc86bec0b0f923fb0716c9" name="a91b2774d64fc86bec0b0f923fb0716c9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a91b2774d64fc86bec0b0f923fb0716c9">&#9670;&#160;</a></span>train()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class NetT &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="structResult.html">Result</a> <a class="el" href="classGradient__Descent.html">Gradient_Descent</a>&lt; NetT &gt;::train </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>batch_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>budget</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>max_iterations</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>verbose</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">override</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Train using SGD. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">batch_size</td><td>Batch size; 0 means full batch. </td></tr>
    <tr><td class="paramname">budget</td><td>Maximum number of samples to consume (0 disables budget stop). </td></tr>
    <tr><td class="paramname">max_iterations</td><td>Maximum optimizer iterations. </td></tr>
    <tr><td class="paramname">verbose</td><td>Enable periodic logging and loss history tracking. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd><a class="el" href="structResult.html" title="Training summary returned by optimizers.">Result</a> Training summary. </dd></dl>
<p>Training loop outline: 1) Fetch a batch from <code><a class="el" href="classPde.html#a13fe3a4b5147b584730716d6fd6a0a18" title="Return the next training batch.">Pde::train_next_batch()</a></code>. 2) Build a leaf tensor <code>x</code> with <code>requires_grad(true)</code> to enable autograd-based PDE operators inside <code><a class="el" href="classPde.html#a02da59985b07868cbe9d6a061b238b85" title="Compute differentiable scalar loss terms.">Pde::losses()</a></code>. 3) Forward pass, compute physics/data losses, backward, SGD step. 4) Track consumed samples via <code>budget_used</code> and stop if <code>budget</code> is reached.</p>
<p>Notes:</p><ul>
<li><code>batch_size == 0</code> means full batch.</li>
<li><code>budget == 0</code> disables the budget stop condition.</li>
</ul>
<p>Training is fully in C++/torch; release the GIL to avoid blocking embedded Python.</p>
<p>Keep a leaf tensor with gradients enabled for PDE derivatives w.r.t. x.</p>
<p>Create a leaf input tensor with gradients enabled for PINN differential operators.</p>
<p>Final loss on full training set.</p>

<p>Implements <a class="el" href="classOptimizer.html#acce62d2367dcc78ecbd510013bcd87c9">Optimizer&lt; NetT &gt;</a>.</p>

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>src/optimizer/<a class="el" href="Gradient__Descent_8hpp_source.html">Gradient_Descent.hpp</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
